{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train\n",
      "(580, 750)\n",
      "(580,)\n",
      "\n",
      ">> test\n",
      "(562, 750)\n",
      "(562,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv1D, MaxPooling1D, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "save_path=\"D:\\\\study\\\\sugyeong_github\\\\FingerVein-Spoofing\\\\data\\\\\"\n",
    "\n",
    "print('>> train')\n",
    "with open(save_path+\"train.pickle\", mode='rb') as f:\n",
    "    trainDict = pickle.load(f)\n",
    "\n",
    "train_X = trainDict['freq_signal']\n",
    "train_y = trainDict['class'].ravel()\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print()\n",
    "print('>> test')\n",
    "with open(save_path+\"test.pickle\", mode='rb') as f:\n",
    "    testDict = pickle.load(f)\n",
    "\n",
    "test_X = testDict['freq_signal']\n",
    "test_y = testDict['class'].ravel()\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 1\n",
      " 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0\n",
      " 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0\n",
      " 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1\n",
      " 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0\n",
      " 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0\n",
      " 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
      " 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1\n",
      " 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0\n",
      " 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "tmp = [[x,y] for x, y in zip(train_X,train_y)]\n",
    "random.shuffle(tmp)\n",
    "train_X = [n[0] for n in tmp]\n",
    "train_y = [n[1] for n in tmp]\n",
    "\n",
    "train_y=np.array(train_y)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data reshape & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(580, 750, 1) (580,)\n"
     ]
    }
   ],
   "source": [
    "train_X=np.reshape(train_X, ( 580,750,1))\n",
    "test_X=np.reshape(test_X, ( 562,750,1))\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (580, 2)\n",
      "(562, 2)\n"
     ]
    }
   ],
   "source": [
    "train_y = to_categorical(train_y,2)\n",
    "test_y = to_categorical(test_y,2)\n",
    "print(type(train_y),train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[250:562])\n",
    "#print(test_y[0:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,val_X,train_y,val_y=train_test_split(train_X,train_y,test_size=0.4,shuffle=True,stratify=train_y,random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_168 (Conv1D)          (None, 374, 8)            32        \n",
      "_________________________________________________________________\n",
      "batch_normalization_168 (Bat (None, 374, 8)            32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 187, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 93, 16)            400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_169 (Bat (None, 93, 16)            64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 46, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 22, 32)            1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_170 (Bat (None, 22, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 22, 32)            1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_171 (Bat (None, 22, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 10, 64)            6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_172 (Bat (None, 10, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 10, 64)            4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_173 (Bat (None, 10, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 4, 64)             12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_174 (Bat (None, 4, 64)             256       \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 4, 64)             4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_175 (Bat (None, 4, 64)             256       \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 40,098\n",
      "Trainable params: 39,410\n",
      "Non-trainable params: 688\n",
      "_________________________________________________________________\n",
      "Train on 348 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.5335 - accuracy: 0.7622\n",
      "Epoch 00001: val_loss improved from inf to 0.97487, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.01-0.97.h5\n",
      "348/348 [==============================] - 4s 11ms/sample - loss: 0.5272 - accuracy: 0.7658 - val_loss: 0.9749 - val_accuracy: 0.4871\n",
      "Epoch 2/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.2844 - accuracy: 0.8963\n",
      "Epoch 00002: val_loss did not improve from 0.97487\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.2738 - accuracy: 0.8994 - val_loss: 2.1071 - val_accuracy: 0.4871\n",
      "Epoch 3/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.2340 - accuracy: 0.9268\n",
      "Epoch 00003: val_loss did not improve from 0.97487\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.2504 - accuracy: 0.9282 - val_loss: 3.4055 - val_accuracy: 0.4871\n",
      "Epoch 4/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.2537 - accuracy: 0.9192\n",
      "Epoch 00004: val_loss did not improve from 0.97487\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.2542 - accuracy: 0.9181 - val_loss: 2.8102 - val_accuracy: 0.4871\n",
      "Epoch 5/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.3739 - accuracy: 0.9024\n",
      "Epoch 00005: val_loss improved from 0.97487 to 0.51271, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.05-0.51.h5\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.3640 - accuracy: 0.9052 - val_loss: 0.5127 - val_accuracy: 0.5948\n",
      "Epoch 6/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0991 - accuracy: 0.9634\n",
      "Epoch 00006: val_loss did not improve from 0.51271\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0959 - accuracy: 0.9655 - val_loss: 4.7097 - val_accuracy: 0.4871\n",
      "Epoch 7/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1957 - accuracy: 0.9421\n",
      "Epoch 00007: val_loss did not improve from 0.51271\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1937 - accuracy: 0.9368 - val_loss: 3.4911 - val_accuracy: 0.5129\n",
      "Epoch 8/100\n",
      "344/348 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9695\n",
      "Epoch 00008: val_loss did not improve from 0.51271\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1287 - accuracy: 0.9698 - val_loss: 1.5519 - val_accuracy: 0.5129\n",
      "Epoch 9/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.9512\n",
      "Epoch 00009: val_loss did not improve from 0.51271\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1715 - accuracy: 0.9511 - val_loss: 0.5535 - val_accuracy: 0.5172\n",
      "Epoch 10/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1389 - accuracy: 0.9649\n",
      "Epoch 00010: val_loss did not improve from 0.51271\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1326 - accuracy: 0.9670 - val_loss: 0.8874 - val_accuracy: 0.5172\n",
      "Epoch 11/100\n",
      "312/348 [=========================>....] - ETA: 0s - loss: 0.2190 - accuracy: 0.9295\n",
      "Epoch 00011: val_loss did not improve from 0.51271\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.2118 - accuracy: 0.9310 - val_loss: 1.0164 - val_accuracy: 0.6078\n",
      "Epoch 12/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.2521 - accuracy: 0.9207\n",
      "Epoch 00012: val_loss did not improve from 0.51271\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.2468 - accuracy: 0.9167 - val_loss: 1.9269 - val_accuracy: 0.5129\n",
      "Epoch 13/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1595 - accuracy: 0.9466\n",
      "Epoch 00013: val_loss improved from 0.51271 to 0.26004, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.13-0.26.h5\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1570 - accuracy: 0.9468 - val_loss: 0.2600 - val_accuracy: 0.8707\n",
      "Epoch 14/100\n",
      "320/348 [==========================>...] - ETA: 0s - loss: 0.0997 - accuracy: 0.9750\n",
      "Epoch 00014: val_loss did not improve from 0.26004\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1397 - accuracy: 0.9684 - val_loss: 0.5440 - val_accuracy: 0.8470\n",
      "Epoch 15/100\n",
      "336/348 [===========================>..] - ETA: 0s - loss: 0.1494 - accuracy: 0.9301\n",
      "Epoch 00015: val_loss improved from 0.26004 to 0.15470, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.15-0.15.h5\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1471 - accuracy: 0.9296 - val_loss: 0.1547 - val_accuracy: 0.9224\n",
      "Epoch 16/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0896 - accuracy: 0.9588\n",
      "Epoch 00016: val_loss improved from 0.15470 to 0.10300, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.16-0.10.h5\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0847 - accuracy: 0.9612 - val_loss: 0.1030 - val_accuracy: 0.9741\n",
      "Epoch 17/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.2289 - accuracy: 0.9726\n",
      "Epoch 00017: val_loss did not improve from 0.10300\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.2215 - accuracy: 0.9713 - val_loss: 0.3532 - val_accuracy: 0.9332\n",
      "Epoch 18/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1622 - accuracy: 0.9756\n",
      "Epoch 00018: val_loss did not improve from 0.10300\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1574 - accuracy: 0.9741 - val_loss: 0.1580 - val_accuracy: 0.9353\n",
      "Epoch 19/100\n",
      "320/348 [==========================>...] - ETA: 0s - loss: 0.1974 - accuracy: 0.9531\n",
      "Epoch 00019: val_loss did not improve from 0.10300\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1851 - accuracy: 0.9569 - val_loss: 0.5393 - val_accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1545 - accuracy: 0.9527\n",
      "Epoch 00020: val_loss improved from 0.10300 to 0.08530, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.20-0.09.h5\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1488 - accuracy: 0.9555 - val_loss: 0.0853 - val_accuracy: 0.9784\n",
      "Epoch 21/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0828 - accuracy: 0.9695\n",
      "Epoch 00021: val_loss did not improve from 0.08530\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0823 - accuracy: 0.9713 - val_loss: 0.3220 - val_accuracy: 0.8836\n",
      "Epoch 22/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0938 - accuracy: 0.9604\n",
      "Epoch 00022: val_loss improved from 0.08530 to 0.03527, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.22-0.04.h5\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0919 - accuracy: 0.9626 - val_loss: 0.0353 - val_accuracy: 0.9849\n",
      "Epoch 23/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0478 - accuracy: 0.9848\n",
      "Epoch 00023: val_loss improved from 0.03527 to 0.02104, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.23-0.02.h5\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0588 - accuracy: 0.9828 - val_loss: 0.0210 - val_accuracy: 0.9914\n",
      "Epoch 24/100\n",
      "320/348 [==========================>...] - ETA: 0s - loss: 0.0493 - accuracy: 0.9812\n",
      "Epoch 00024: val_loss did not improve from 0.02104\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0495 - accuracy: 0.9828 - val_loss: 0.0370 - val_accuracy: 0.9914\n",
      "Epoch 25/100\n",
      "312/348 [=========================>....] - ETA: 0s - loss: 0.0363 - accuracy: 0.9840\n",
      "Epoch 00025: val_loss did not improve from 0.02104\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0375 - accuracy: 0.9828 - val_loss: 0.0432 - val_accuracy: 0.9784\n",
      "Epoch 26/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0316 - accuracy: 0.9909\n",
      "Epoch 00026: val_loss did not improve from 0.02104\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0307 - accuracy: 0.9914 - val_loss: 0.1007 - val_accuracy: 0.9784\n",
      "Epoch 27/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0680 - accuracy: 0.9756\n",
      "Epoch 00027: val_loss did not improve from 0.02104\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0793 - accuracy: 0.9713 - val_loss: 0.0714 - val_accuracy: 0.9871\n",
      "Epoch 28/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0693 - accuracy: 0.9726 ETA: 0s - loss: 0.0789 - accura\n",
      "Epoch 00028: val_loss did not improve from 0.02104\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0675 - accuracy: 0.9741 - val_loss: 0.0311 - val_accuracy: 0.9828\n",
      "Epoch 29/100\n",
      "320/348 [==========================>...] - ETA: 0s - loss: 0.0539 - accuracy: 0.9750\n",
      "Epoch 00029: val_loss did not improve from 0.02104\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0499 - accuracy: 0.9770 - val_loss: 0.0631 - val_accuracy: 0.9871\n",
      "Epoch 30/100\n",
      "320/348 [==========================>...] - ETA: 0s - loss: 0.0515 - accuracy: 0.9781\n",
      "Epoch 00030: val_loss improved from 0.02104 to 0.00435, saving model to D:\\study\\sugyeong_github\\FingerVein-Spoofing\\model\\result\\model.30-0.00.h5\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0474 - accuracy: 0.9799 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0377 - accuracy: 0.9909\n",
      "Epoch 00031: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0372 - accuracy: 0.9914 - val_loss: 0.1884 - val_accuracy: 0.9612\n",
      "Epoch 32/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0398 - accuracy: 0.9848\n",
      "Epoch 00032: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0377 - accuracy: 0.9856 - val_loss: 0.0315 - val_accuracy: 0.9871\n",
      "Epoch 33/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1743 - accuracy: 0.9619\n",
      "Epoch 00033: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1665 - accuracy: 0.9641 - val_loss: 0.1605 - val_accuracy: 0.9741\n",
      "Epoch 34/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0503 - accuracy: 0.9909\n",
      "Epoch 00034: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0474 - accuracy: 0.9914 - val_loss: 0.0876 - val_accuracy: 0.9914\n",
      "Epoch 35/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.3813 - accuracy: 0.8780\n",
      "Epoch 00035: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.3634 - accuracy: 0.8851 - val_loss: 0.3045 - val_accuracy: 0.9138\n",
      "Epoch 36/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9512\n",
      "Epoch 00036: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1272 - accuracy: 0.9540 - val_loss: 0.0173 - val_accuracy: 0.9957\n",
      "Epoch 37/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.3293 - accuracy: 0.9253\n",
      "Epoch 00037: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.3159 - accuracy: 0.9239 - val_loss: 0.0907 - val_accuracy: 0.9655\n",
      "Epoch 38/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1563 - accuracy: 0.9192\n",
      "Epoch 00038: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1557 - accuracy: 0.9210 - val_loss: 0.0721 - val_accuracy: 0.9784\n",
      "Epoch 39/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.0669 - accuracy: 0.9741\n",
      "Epoch 00039: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.0638 - accuracy: 0.9756 - val_loss: 0.0447 - val_accuracy: 0.9914\n",
      "Epoch 40/100\n",
      "328/348 [===========================>..] - ETA: 0s - loss: 0.1607 - accuracy: 0.9451\n",
      "Epoch 00040: val_loss did not improve from 0.00435\n",
      "348/348 [==============================] - 1s 2ms/sample - loss: 0.1549 - accuracy: 0.9454 - val_loss: 0.0303 - val_accuracy: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20c06292ec8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cnn():\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D(activation='relu',input_shape=(750,1),strides=2,filters=8,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    \n",
    "    model.add(Conv1D(activation='relu',strides=2,filters=16,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    \n",
    "    model.add(Conv1D(activation='relu',strides=2,filters=32,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(activation='relu',strides=1,filters=32,kernel_size=1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(activation='relu',strides=2,filters=64,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(activation='relu',strides=1,filters=64,kernel_size=1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(activation='relu',strides=2,filters=64,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(activation='relu',strides=1,filters=64,kernel_size=1))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling1D(pool_size = 2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='sigmoid'))\n",
    "    \n",
    "    Adam= optimizers.Adam(lr=1e-2, decay=1e-4)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam,metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# def lr_scheduler(epoch, lr):\n",
    "#     decay_rate = 0.1\n",
    "#     decay_step = 90\n",
    "#     if epoch % decay_step == 0 and epoch:\n",
    "#         return lr * decay_rate\n",
    "#     return lr\n",
    "\n",
    "\n",
    "model_path=\"D:\\\\study\\\\sugyeong_github\\\\FingerVein-Spoofing\\\\model\\\\result\\\\\"\n",
    "\n",
    "model=cnn()\n",
    "model.summary()\n",
    "my_callbacks = [\n",
    "   # tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=model_path+'model.{epoch:02d}-{val_loss:.2f}.h5',monitor='val_loss',   # val_loss 값이 개선되었을때 호출됩니다\n",
    "                             verbose=1,            # 로그를 출력합니다\n",
    "                             save_best_only=True,  # 가장 best 값만 저장합니다\n",
    "                             mode='auto'           # auto는 알아서 best를 찾습니다. min/max\n",
    "                            )\n",
    "]\n",
    "model.fit(train_X,train_y,epochs=100,callbacks=my_callbacks,verbose=1,batch_size=8,validation_data=(val_X,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "562/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 777us/sample - loss: 10.9615 - accuracy: 0.7473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss and Accuracy ->  [4.21190996953598, 0.74733096]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(model_path+'model.30-0.00.h5')\n",
    "\n",
    "performance_test = model.evaluate(test_X, test_y, batch_size=8)\n",
    "print('Test Loss and Accuracy -> ', performance_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능평가 결과\n",
    "- Adam validation 0.4 로 했을때 model.06-0.16.h5\n",
    "    - loss & acc => 0.49 & 0.89\n",
    "- Adam lr 0.001했을때 \n",
    "    - batch 2 : 4.7021 / 0.7616\n",
    "    - batch 4 : 4.3471 / 0.7447\n",
    "- Adam lr 0.01 \n",
    "    - batch 1 : 0.6412 / 0.86\n",
    "    - batch 2 : 0.4317 / 0.8915\n",
    "    - batch 4 : 1.0917 / 0.8692\n",
    "    - batch 8 : 10.9615 / 0.7473\n",
    "    \n",
    "    \n",
    "    \n",
    "- SGD로 했을때 model.04-0.20_SGD.h5\n",
    "    - 1.05 & 0.71\n",
    "- SGD ,lr=0.01 했을때 \n",
    "    - batch 1 : 0.7558 / 0.7562\n",
    "    - batch 2: 4.3121 / 0.7553\n",
    "- SGD ,lr=0.001 했을때 \n",
    "    - 성능안좋음\n",
    "\n",
    "- RMSprop > 성능안좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.5166988e-06 9.9999988e-01]\n",
      " [1.0148901e-01 9.5175254e-01]\n",
      " [1.2446346e-06 1.0000000e+00]\n",
      " [1.3343046e-02 9.9756515e-01]\n",
      " [5.7274121e-04 9.9993801e-01]\n",
      " [1.0907582e-03 9.9983251e-01]\n",
      " [1.9844039e-03 9.9975520e-01]\n",
      " [9.9984384e-01 1.6689177e-05]\n",
      " [9.9480671e-01 1.8463597e-03]\n",
      " [2.8287157e-04 9.9997425e-01]\n",
      " [9.9119824e-01 3.8491113e-03]\n",
      " [4.5966062e-05 9.9999702e-01]\n",
      " [2.9248154e-02 9.9138016e-01]\n",
      " [2.3068315e-01 8.6303610e-01]\n",
      " [1.4719785e-03 9.9979132e-01]\n",
      " [5.5057731e-06 9.9999988e-01]\n",
      " [0.0000000e+00 1.0000000e+00]\n",
      " [2.8240223e-07 1.0000000e+00]\n",
      " [6.1303942e-04 9.9992800e-01]\n",
      " [9.0250809e-04 9.9990261e-01]\n",
      " [7.7552795e-06 9.9999976e-01]\n",
      " [2.4267688e-06 9.9999988e-01]\n",
      " [7.8118426e-08 1.0000000e+00]\n",
      " [8.1564259e-08 1.0000000e+00]\n",
      " [4.7615645e-06 9.9999988e-01]\n",
      " [2.6856247e-05 9.9999869e-01]\n",
      " [9.0645878e-03 9.9824202e-01]\n",
      " [9.6986247e-03 9.9764758e-01]\n",
      " [1.0074449e-01 9.5912808e-01]\n",
      " [9.7225589e-01 1.2073341e-02]\n",
      " [7.8419000e-01 2.0631687e-01]\n",
      " [9.5776886e-01 1.3633308e-02]]\n",
      "============================\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "result=model.predict(test_X,batch_size=1)\n",
    "print(result[530:])\n",
    "print(\"============================\")\n",
    "print(test_y[530:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29 samples, validate on 551 samples\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 40ms/sample - loss: 0.6785 - accuracy: 0.9660 - val_loss: 0.6962 - val_accuracy: 0.4610\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 25ms/sample - loss: 0.6431 - accuracy: 1.0000 - val_loss: 0.7017 - val_accuracy: 0.4610\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 24ms/sample - loss: 0.5984 - accuracy: 1.0000 - val_loss: 0.7116 - val_accuracy: 0.4610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bc2cddb308>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class DNN(models.Sequential):\n",
    "    def __init__(self, Nout):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.add(layers.Dense(100, activation='relu', input_shape=(750,), name='Hidden-1'))\n",
    "        self.add(layers.Dense(50, activation='relu', name='Hidden-2'))\n",
    "        self.add(layers.Dense(Nout, activation='sigmoid'))\n",
    "        self.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#def main():\n",
    "number_of_class = 2\n",
    "Nout = number_of_class\n",
    "\n",
    "model = (DNN(Nout))\n",
    "    \n",
    "#history = model.fit(train_X, train_y, epochs=50, batch_size=16, validation_split=0.01)\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5')\n",
    "]\n",
    "model.fit(train_X, train_y, epochs=10, callbacks=my_callbacks, batch_size = 1, validation_split=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "562/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 988us/sample - loss: 0.1594 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss and Accuracy ->  [0.15935717464766044, 1.0]\n"
     ]
    }
   ],
   "source": [
    "performance_test = model.evaluate(test_X, test_y, batch_size=1)\n",
    "print('Test Loss and Accuracy -> ', performance_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spoofing",
   "language": "python",
   "name": "spoofing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
